{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2., inf,  4.,  5.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ar = np.array([1., 2., 3., 4., 5.])\n",
    "\n",
    "ar[2] = np.nan\n",
    "\n",
    "np.nan_to_num(ar, nan=np.inf, copy=False)\n",
    "ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(380.4066)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# timeit\n",
    "\n",
    "N = 100\n",
    "M = 4\n",
    "v = torch.randn(N)\n",
    "v\n",
    "K = torch.randn(N, N).abs()\n",
    "\n",
    "A = torch.randn(N, M)\n",
    "\n",
    "@torch.jit.script\n",
    "def f1(K,A):\n",
    "    f_hat = K @ A\n",
    "    f_hat_K_tch_inv = torch.linalg.solve(K, f_hat)\n",
    "    return torch.trace(f_hat.T @ f_hat_K_tch_inv)\n",
    "\n",
    "@torch.jit.script\n",
    "def f2(K,A):\n",
    "    f_hat = K @ A\n",
    "    return torch.trace(f_hat.T @ torch.inverse(K) @ f_hat)\n",
    "\n",
    "def f3(K,A):\n",
    "    f_hat = K @ A\n",
    "    f_hat_K_tch_inv = torch.linalg.solve(K, f_hat)\n",
    "    return torch.trace(f_hat.T @ f_hat_K_tch_inv)\n",
    "\n",
    "def f4(K,A):\n",
    "    f_hat = K @ A\n",
    "    return torch.trace(f_hat.T @ torch.inverse(K) @ f_hat)\n",
    "f1(K,A)\n",
    "\n",
    "# %timeit f1(K,A)\n",
    "# %timeit f2(K,A)\n",
    "# %timeit f3(K,A)\n",
    "# %timeit f4(K,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\queim\\micromambaenv\\envs\\stpy39\\lib\\site-packages\\torch\\jit\\_script.py:1138: UserWarning: Warning: monkeytype is not installed. Please install https://github.com/Instagram/MonkeyType to enable Profile-Directed Typing in TorchScript. Refer to https://github.com/Instagram/MonkeyType/blob/master/README.rst to install MonkeyType. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-15.0465, dtype=torch.float64)\n",
      "Restart 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\queim\\micromambaenv\\envs\\stpy39\\lib\\site-packages\\torch\\jit\\_script.py:1138: UserWarning: Warning: monkeytype is not installed. Please install https://github.com/Instagram/MonkeyType to enable Profile-Directed Typing in TorchScript. Refer to https://github.com/Instagram/MonkeyType/blob/master/README.rst to install MonkeyType. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.3259, dtype=torch.float64)\n",
      "initial fval: 194.6078\n",
      "tensor(-7.0117, dtype=torch.float64)\n",
      "tensor(-7.3259, dtype=torch.float64)\n",
      "tensor(-7.1986, dtype=torch.float64)\n",
      "tensor(-7.3260, dtype=torch.float64)\n",
      "tensor(-7.0083, dtype=torch.float64)\n",
      "tensor(-7.0109, dtype=torch.float64)\n",
      "iter   1 - fval: 192.5192\n",
      "Maximum number of iterations has been exceeded.\n",
      "         Current function value: 192.519225\n",
      "         Iterations: 1\n",
      "         Function evaluations: 7\n",
      "0 Cost: 192.519, Params: 4.558496\n",
      "Best index: 0\n",
      "Kernel description:\n",
      "\n",
      "\tkernel: squared_exponential\n",
      "\toperation: -\n",
      "\tkappa=1.0\n",
      "\tgroup=[0]\n",
      "\toffset=0.0\n",
      "\tgamma=tensor([4.5585], dtype=torch.float64)\n",
      "lambda=0.001\n",
      "tensor(4.7338, dtype=torch.float64)\n",
      "tensor(4.7453, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from stpy.continuous_processes.gauss_procc import GaussianProcess\n",
    "from stpy.helpers.helper import interval\n",
    "\n",
    "\n",
    "# Parameters\n",
    "N = 5\n",
    "n = 256\n",
    "d = 1\n",
    "eps = 0.01\n",
    "s = 1\n",
    "gamma = 0.1\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Generate random data\n",
    "x = torch.rand(N, d).double() * 2 - 1\n",
    "xtest = torch.from_numpy(interval(n, d, L_infinity_ball=1))\n",
    "\n",
    "# True Gaussian process\n",
    "GP_true = GaussianProcess(gamma=gamma, kernel_name=\"squared_exponential\", d=d)\n",
    "ytest = GP_true.sample(xtest)\n",
    "GP_true.fit_gp(xtest, ytest)\n",
    "y = GP_true.mean(x).clone()\n",
    "\n",
    "# New data points\n",
    "xnew = x[0, :].view(1, 1) + eps\n",
    "ynew = y[0, 0].view(1, 1) + 1\n",
    "\n",
    "# Combined data\n",
    "x2 = torch.vstack([x, xnew])\n",
    "y2 = torch.vstack([y, ynew])\n",
    "\n",
    "# Initialize models\n",
    "gp_cvx = GaussianProcess(gamma=gamma, kernel_name=\"squared_exponential\", d=d, loss='huber', huber_delta=1.5)\n",
    "# gp_torch = GaussianProcess(gamma=gamma, kernel_name=\"squared_exponential\", d=d, loss='huber_torch', huber_delta=1.5)\n",
    "gp_st = GaussianProcess(gamma=gamma, kernel_name=\"squared_exponential\", d=d, loss='studentT', lam=0.005)\n",
    "\n",
    "gp_st.fit_gp(x2, y2)\n",
    "from torch.profiler import profile, record_function, ProfilerActivity \n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=False) as prof:\n",
    "    # Fit models\n",
    "    gp_st.optimize_params(type=\"bandwidth\", restarts=1, verbose=True, optimizer='pytorch-minimize', scale=1., weight=1., maxiter=1)\n",
    "\n",
    "# Fit models\n",
    "# %timeit gp_cvx.fit_gp(x2, y2)\n",
    "# %timeit gp_torch.fit_gp(x2, y2)\n",
    "\n",
    "# mean_cvx = gp_cvx.mean(xtest)\n",
    "# mean_torch = gp_torch.mean(xtest)\n",
    "mean_st = gp_st.mean(xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                          Optimizer.step#Minimizer.step        44.22%        3.315s        98.08%        7.353s     919.067ms             8  \n",
      "                                              aten::dot         6.11%     458.381ms         7.86%     589.061ms       3.897us        151173  \n",
      "                                           aten::select         6.49%     486.507ms         7.59%     569.136ms       2.586us        220068  \n",
      "                                              aten::mul         5.75%     430.789ms         5.96%     446.719ms       2.361us        189192  \n",
      "                                             aten::add_         3.73%     279.476ms         3.73%     279.476ms       1.776us        157397  \n",
      "                                             aten::item         2.36%     176.704ms         2.97%     222.611ms       1.385us        160683  \n",
      "autograd::engine::evaluate_function: struct torch::j...         0.11%       8.038ms         2.86%     214.589ms     155.612us          1379  \n",
      "                                                forward         0.53%      39.543ms         2.81%     210.428ms     152.484us          1380  \n",
      "struct torch::jit::`anonymous namespace'::Differenti...         0.20%      15.190ms         2.71%     202.872ms     147.116us          1379  \n",
      "                                              aten::sub         2.68%     200.925ms         2.70%     202.236ms       2.405us         84100  \n",
      "                                          <backward op>         0.54%      40.203ms         2.50%     187.683ms     136.101us          1379  \n",
      "                                            aten::copy_         2.32%     174.229ms         2.32%     174.229ms       1.697us        102653  \n",
      "                                              aten::neg         2.02%     151.635ms         2.02%     151.635ms       1.900us         79825  \n",
      "                                              aten::div         1.17%      88.066ms         2.02%     151.076ms       5.858us         25790  \n",
      "                                               aten::to         0.33%      25.101ms         1.75%     130.891ms       5.328us         24567  \n",
      "      autograd::engine::evaluate_function: DivBackward0         0.31%      23.584ms         1.69%     126.591ms      17.558us          7210  \n",
      "                                           <forward op>         0.21%      15.809ms         1.65%     123.992ms      89.849us          1380  \n",
      "                                            aten::empty         1.57%     117.533ms         1.57%     117.533ms       0.704us        166905  \n",
      "                                         aten::_to_copy         0.78%      58.679ms         1.41%     105.790ms       5.277us         20046  \n",
      "                                       aten::as_strided         1.41%     105.698ms         1.41%     105.698ms       0.400us        264120  \n",
      "                                               aten::mm         1.34%     100.567ms         1.37%     102.978ms      10.304us          9994  \n",
      "                                              aten::add         1.00%      75.022ms         1.34%     100.142ms       5.368us         18657  \n",
      "                                           DivBackward0         0.33%      24.458ms         1.33%      99.636ms      13.819us          7210  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.20%      14.876ms         1.31%      98.106ms      32.293us          3038  \n",
      "      autograd::engine::evaluate_function: MulBackward0         0.43%      32.082ms         1.25%      93.403ms      10.675us          8750  \n",
      "                Optimizer.zero_grad#Minimizer.zero_grad         1.19%      88.903ms         1.19%      88.903ms      64.469us          1379  \n",
      "                                              aten::cat         0.71%      52.940ms         1.08%      80.759ms      14.183us          5694  \n",
      "                                           aten::narrow         0.48%      35.735ms         1.07%      80.041ms       3.774us         21209  \n",
      "                                            MmBackward0         0.21%      15.685ms         1.05%      78.522ms      25.846us          3038  \n",
      "                                           aten::matmul         0.15%      11.117ms         0.99%      73.865ms      13.302us          5553  \n",
      "    autograd::engine::evaluate_function: TraceBackward0         0.04%       3.294ms         0.91%      68.167ms      49.183us          1386  \n",
      "                                         TraceBackward0         0.05%       3.406ms         0.87%      64.873ms      46.806us          1386  \n",
      "                                   aten::trace_backward         0.14%      10.656ms         0.82%      61.468ms      44.349us          1386  \n",
      "                                            aten::slice         0.64%      47.846ms         0.79%      59.458ms       2.470us         24075  \n",
      "                                           MulBackward0         0.29%      21.982ms         0.73%      55.018ms       6.288us          8750  \n",
      "                                            aten::chunk         0.08%       5.745ms         0.72%      54.102ms      19.461us          2780  \n",
      "                                            aten::split         0.28%      20.824ms         0.65%      48.358ms      17.395us          2780  \n",
      "                                              aten::abs         0.39%      29.087ms         0.61%      46.100ms       4.863us          9479  \n",
      "                              aten::_local_scalar_dense         0.61%      45.907ms         0.61%      45.907ms       0.286us        160683  \n",
      "                                           aten::linear         0.08%       6.296ms         0.54%      40.476ms      29.330us          1380  \n",
      "      autograd::engine::evaluate_function: PowBackward0         0.07%       4.963ms         0.54%      40.167ms      24.418us          1645  \n",
      "                                                aten::t         0.28%      21.267ms         0.54%      40.152ms       4.401us          9124  \n",
      "      autograd::engine::evaluate_function: AddBackward0         0.37%      27.520ms         0.53%      40.052ms       4.087us          9800  \n",
      "                                               aten::gt         0.31%      23.015ms         0.53%      39.782ms       9.179us          4334  \n",
      "      autograd::engine::evaluate_function: CatBackward0         0.06%       4.460ms         0.48%      35.752ms      23.645us          1512  \n",
      "                                           PowBackward0         0.11%       8.282ms         0.47%      35.024ms      21.291us          1645  \n",
      "                                           aten::arange         0.23%      17.269ms         0.45%      34.017ms      12.089us          2814  \n",
      "autograd::engine::evaluate_function: LgammaBackward0...         0.11%       8.338ms         0.45%      33.906ms      12.170us          2786  \n",
      "     autograd::engine::evaluate_function: SqrtBackward0         0.05%       3.812ms         0.45%      33.697ms      22.183us          1519  \n",
      "                                                  chunk         0.04%       3.228ms         0.44%      32.804ms      23.771us          1380  \n",
      "                                           CatBackward0         0.09%       6.602ms         0.42%      31.291ms      20.695us          1512  \n",
      "                                          SqrtBackward0         0.09%       6.563ms         0.40%      29.884ms      19.673us          1519  \n",
      "                                              aten::pow         0.32%      24.057ms         0.40%      29.858ms       8.993us          3320  \n",
      "                                      aten::index_fill_         0.18%      13.354ms         0.35%      26.463ms       9.547us          2772  \n",
      "                                            aten::fill_         0.34%      25.743ms         0.34%      25.743ms       0.166us        155504  \n",
      "                                        LgammaBackward0         0.13%       9.781ms         0.34%      25.567ms       9.177us          2786  \n",
      "                                       aten::is_nonzero         0.15%      11.536ms         0.34%      25.461ms       2.470us         10309  \n",
      "                                             aten::view         0.33%      24.611ms         0.33%      24.611ms       1.838us         13387  \n",
      "                                         aten::isfinite         0.09%       7.049ms         0.31%      23.179ms      20.368us          1138  \n",
      "      autograd::engine::evaluate_function: LogBackward0         0.12%       8.781ms         0.30%      22.176ms       7.615us          2912  \n",
      "                                              aten::sum         0.23%      17.238ms         0.25%      19.070ms      12.842us          1485  \n",
      "                                        aten::transpose         0.19%      13.980ms         0.25%      18.944ms       2.069us          9154  \n",
      "                                               aten::le         0.24%      18.036ms         0.25%      18.653ms       3.173us          5878  \n",
      "                                            aten::where         0.22%      16.246ms         0.24%      17.914ms       6.495us          2758  \n",
      "                               aten::linalg_vector_norm         0.22%      16.293ms         0.24%      17.776ms       7.838us          2268  \n",
      "                                    aten::empty_strided         0.24%      17.745ms         0.24%      17.745ms       0.823us         21574  \n",
      "      autograd::engine::evaluate_function: SubBackward0         0.09%       6.612ms         0.23%      17.254ms       5.897us          2926  \n",
      "    autograd::engine::evaluate_function: SplitBackward0         0.05%       3.724ms         0.22%      16.601ms      10.879us          1526  \n",
      "autograd::engine::evaluate_function: SoftplusBackwar...         0.01%     812.422us         0.22%      16.127ms      63.998us           252  \n",
      "                              SoftplusBackwardBackward0         0.03%       2.547ms         0.20%      15.315ms      60.774us           252  \n",
      "                                         aten::softplus         0.20%      15.241ms         0.20%      15.241ms       5.439us          2802  \n",
      "                                           aten::lgamma         0.20%      14.836ms         0.20%      14.836ms       5.325us          2786  \n",
      "                                            aten::clone         0.08%       6.117ms         0.19%      14.089ms       9.090us          1550  \n",
      "                                          aten::permute         0.17%      12.491ms         0.19%      14.044ms       5.043us          2785  \n",
      "                                           LogBackward0         0.07%       5.099ms         0.18%      13.214ms       4.538us          2912  \n",
      "                                              aten::log         0.17%      13.118ms         0.17%      13.118ms       4.709us          2786  \n",
      "                                           aten::detach         0.07%       4.895ms         0.17%      13.113ms       3.107us          4220  \n",
      "                                         SplitBackward0         0.06%       4.731ms         0.17%      12.877ms       8.439us          1526  \n",
      "                                          aten::reshape         0.09%       6.890ms         0.17%      12.709ms       3.819us          3328  \n",
      "                                               aten::lt         0.13%       9.863ms         0.16%      12.197ms       4.524us          2696  \n",
      "autograd::engine::evaluate_function: PermuteBackward...         0.05%       3.865ms         0.16%      11.963ms       8.632us          1386  \n",
      "      autograd::engine::evaluate_function: SumBackward0         0.04%       2.761ms         0.16%      11.693ms       8.479us          1379  \n",
      "                                              aten::exp         0.15%      11.564ms         0.15%      11.564ms       4.167us          2775  \n",
      "                                             aten::sqrt         0.15%      11.424ms         0.15%      11.424ms       6.979us          1637  \n",
      "autograd::engine::evaluate_function: torch::autograd...         0.04%       2.874ms         0.15%      11.363ms       7.515us          1512  \n",
      "                                          aten::numpy_T         0.03%       2.325ms         0.15%      11.210ms       8.042us          1394  \n",
      "                                          aten::view_as         0.06%       4.218ms         0.14%      10.682ms       2.724us          3921  \n",
      "                                           SubBackward0         0.06%       4.377ms         0.14%      10.642ms       3.637us          2926  \n",
      "                                            aten::zeros         0.09%       6.427ms         0.13%      10.082ms       6.577us          1533  \n",
      "                                        aten::ones_like         0.06%       4.609ms         0.13%       9.798ms       6.450us          1519  \n",
      "                                          aten::digamma         0.13%       9.748ms         0.13%       9.748ms       3.499us          2786  \n",
      "                                           SumBackward0         0.04%       2.730ms         0.12%       8.932ms       6.477us          1379  \n",
      "                                       aten::empty_like         0.08%       5.973ms         0.11%       8.575ms       2.802us          3060  \n",
      "                        torch::autograd::AccumulateGrad         0.05%       4.104ms         0.11%       8.489ms       6.156us          1379  \n",
      "                                           aten::__or__         0.04%       2.964ms         0.11%       8.452ms       7.479us          1130  \n",
      "                                                 detach         0.11%       8.218ms         0.11%       8.218ms       1.947us          4220  \n",
      "                                       PermuteBackward0         0.04%       2.950ms         0.11%       8.098ms       5.843us          1386  \n",
      "                                            aten::trace         0.09%       6.442ms         0.11%       7.963ms       5.745us          1386  \n",
      "                                              aten::max         0.09%       6.795ms         0.10%       7.683ms       6.745us          1139  \n",
      "                                           aten::expand         0.07%       5.462ms         0.08%       6.264ms       4.496us          1393  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 7.497s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prof.export_chrome_trace(\"trace.json\")\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_cvx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(xtest, \u001b[43mmean_cvx\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcvx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# plt.plot(xtest, mean_torch, label='torch')\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x, y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_cvx' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "plt.plot(xtest, mean_cvx, label='cvx')\n",
    "# plt.plot(xtest, mean_torch, label='torch')\n",
    "plt.plot(x, y, 'o', label='data')\n",
    "plt.plot(xnew, ynew, 'o', label='new data')\n",
    "\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
