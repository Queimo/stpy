{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:15:39.707658121Z",
     "start_time": "2024-10-30T09:15:36.425378977Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stpy.kernels import KernelFunction\n",
    "from stpy.continuous_processes.nystrom_fea import NystromFeatures\n",
    "from stpy.helpers.helper import interval_torch\n",
    "import torch\n",
    "from torch.nn import Sequential, Linear\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from mtevi import EvidenceRegularizer, EvidentialnetMarginalLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:15:39.756799028Z",
     "start_time": "2024-10-30T09:15:39.710675819Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kernel = KernelFunction(kernel_name=\"matern\", gamma=0.5, nu = 3.5, d = 1) #TODO check why runtime is slow\n",
    "kernel = KernelFunction(kernel_name=\"squared_exponential\", gamma=0.5, kappa=6.6,d = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:15:39.757264717Z",
     "start_time": "2024-10-30T09:15:39.741149326Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.random.seed(10)\n",
    "# torch.manual_seed(10)\n",
    "\n",
    "# num_factor = 10\n",
    "\n",
    "# X = np.concatenate((np.linspace(-3, 6, 20*num_factor), np.linspace(6, 10, 5*num_factor)))\n",
    "# rand_num = [6,3]\n",
    "\n",
    "# F = lambda X: np.sin(X*4)**3 + (X**2)/10 -2.8 -0.0274\n",
    "# eps = lambda X: np.random.randn(*X.shape)*0.1 + 2*np.random.normal(scale=np.abs(7-np.abs(X))*0.05)\n",
    "# Y = F(X) + eps(X)\n",
    "\n",
    "# X = np.expand_dims(X, 1)\n",
    "# dim = len(X[0])\n",
    "\n",
    "# sparse_idx_A = 21*num_factor\n",
    "# sparse_idx_B = 24*num_factor\n",
    "\n",
    "# s = 4*num_factor\n",
    "# e = 13*num_factor\n",
    "\n",
    "# X_t = np.concatenate((X[s:e], X[sparse_idx_A:sparse_idx_B]))\n",
    "# X_v = np.concatenate((X[:s], X[e:sparse_idx_A], X[sparse_idx_B:]))\n",
    "# Y_t = np.concatenate((Y[s:e], Y[sparse_idx_A:sparse_idx_B]))\n",
    "# Y_v = np.concatenate((Y[:s], Y[e:sparse_idx_A], Y[sparse_idx_B:]))\n",
    "\n",
    "# # outliers 30 points\n",
    "# n_outliers = 5\n",
    "# # X_outliers = np.linspace(-1,3,n_outliers)\n",
    "# # random x values between -1 and 3\n",
    "# X_outliers = np.random.uniform(-1,3,n_outliers)\n",
    "# density_multiplier = 2\n",
    "# X_outliers = np.concatenate([X_outliers]*density_multiplier)\n",
    "# Y_outliers = np.sin(X_outliers*4)**3 + (X_outliers**2)/10 + np.random.randn(*X_outliers.shape)*0.1 + 2*np.random.normal(scale=np.abs(7-np.abs(X_outliers))*0.05) + 5\n",
    "# X_outliers = np.expand_dims(X_outliers, 1)  # Expand dimensions to match X_t\n",
    "\n",
    "# X_t = np.concatenate((X_t, X_outliers))\n",
    "# Y_t = np.concatenate((Y_t, Y_outliers))\n",
    "\n",
    "# Y_t = np.expand_dims(Y_t, 1)\n",
    "# Y_v = np.expand_dims(Y_v, 1)\n",
    "\n",
    "# x_tr = torch.tensor(X_t, dtype=torch.float64)\n",
    "# y_tr = torch.tensor(Y_t, dtype=torch.float64)\n",
    "# x_tr.shape, y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "\n",
    "num_factor = 10\n",
    "\n",
    "X = np.linspace(-3, 10, 100)\n",
    "\n",
    "F = lambda X: np.sin(X*4)**3 + (X**2)/10 -2.8 -0.0274\n",
    "eps = lambda X: np.random.randn(*X.shape)*0.1 + 2*np.random.normal(scale=np.abs(7-np.abs(X))*0.05)\n",
    "Y = F(X) + eps(X)\n",
    "\n",
    "X = np.expand_dims(X, 1)\n",
    "\n",
    "# outliers 30 points\n",
    "n_outliers = 5\n",
    "X_outliers = np.random.uniform(-3,10,n_outliers)\n",
    "\n",
    "density_multiplier = 1\n",
    "X_outliers = np.concatenate([X_outliers]*density_multiplier)\n",
    "Y_outliers = F(X_outliers) + eps(X_outliers) + 5\n",
    "X_outliers = np.expand_dims(X_outliers, 1)  # Expand dimensions to match X_t\n",
    "\n",
    "X_t = np.concatenate((X, X_outliers))\n",
    "Y_t = np.concatenate((Y, Y_outliers))\n",
    "\n",
    "Y_t = np.expand_dims(Y_t, 1)\n",
    "\n",
    "x_tr = torch.tensor(X_t, dtype=torch.float64)\n",
    "y_tr = torch.tensor(Y_t, dtype=torch.float64)\n",
    "x_tr.shape, y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:15:43.397250555Z",
     "start_time": "2024-10-30T09:15:43.389167732Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ShallowModel(Sequential):\n",
    "    def __init__(self, input_dim, emb_dim ,output_dim):\n",
    "        super(ShallowModel, self).__init__()\n",
    "        \n",
    "        self.embedding = NystromFeatures(kernel, m=emb_dim, approx=\"nothing\")\n",
    "        self.emb_fit = False\n",
    "        m = self.embedding.get_m()\n",
    "        self.dense = Linear(m, output_dim, bias=False).double()\n",
    "    \n",
    "    def phi(self, x):\n",
    "        if not self.emb_fit:\n",
    "            # careful: only fit on first batch\n",
    "            self.embedding.fit_gp(x,None, eps=0.1)\n",
    "            self.emb_fit = True\n",
    "            \n",
    "        return self.embedding.embed(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.phi(x)\n",
    "        y = self.dense(h)\n",
    "        return y\n",
    "\n",
    "model = ShallowModel(input_dim=x_tr.shape[1], emb_dim=x_tr.shape[0], output_dim=Y_t.shape[1])\n",
    "huber_model = ShallowModel(input_dim=x_tr.shape[1], emb_dim=x_tr.shape[0], output_dim=Y_t.shape[1])\n",
    "\n",
    "class AminiModel(ShallowModel):\n",
    "    def __init__(self, input_dim, emb_dim, output_dim):\n",
    "        super(AminiModel, self).__init__(input_dim, emb_dim, output_dim * 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.phi(x)\n",
    "        output = self.dense(h)\n",
    "        mu, logv, logalpha, logbeta = torch.chunk(output, 4, dim=-1)\n",
    "        v = torch.nn.functional.softplus(logv)\n",
    "        alpha = torch.nn.functional.softplus(logalpha) + 1\n",
    "        beta = torch.nn.functional.softplus(logbeta)\n",
    "        return torch.cat([mu, v, alpha, beta], dim=-1)\n",
    "\n",
    "amini_model = AminiModel(input_dim=x_tr.shape[1], emb_dim=x_tr.shape[0], output_dim=Y_t.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.Adam(amini_model.parameters(), lr=0.01)\n",
    "\n",
    "objective = EvidentialnetMarginalLikelihood()\n",
    "reg = EvidenceRegularizer(factor=0.0001)\n",
    "\n",
    "def amini_loss(x,y,amini_model):\n",
    "    output = amini_model(x)\n",
    "    gamma, nu, alpha, beta = torch.chunk(output, 4, dim=-1)\n",
    "    y = y.double()\n",
    "    nll = (objective(gamma,nu,alpha,beta,y)).mean()\n",
    "    reg_loss = (reg(gamma, nu, alpha, beta, y)).mean()\n",
    "    return nll + reg_loss\n",
    "    \n",
    "\n",
    "#set_postfix loss print\n",
    "pbar = tqdm.tqdm(range(1000))\n",
    "for i in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    loss = amini_loss(x_tr, y_tr, amini_model)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_postfix(loss=f'{loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:34:53.928678339Z",
     "start_time": "2024-10-30T09:34:53.455778595Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def loss_fn(x,y,model):\n",
    "    y_hat = model(x)\n",
    "    # likelihood + prior\n",
    "    return torch.mean((y_hat - y)**2) + 0.002*torch.sum(model.dense.weight**2)\n",
    "\n",
    "#set_postfix loss print\n",
    "pbar = tqdm.tqdm(range(1000))\n",
    "for i in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(x_tr,y_tr,model)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_postfix(loss=f'{loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(huber_model.parameters(), lr=0.01)\n",
    "\n",
    "def huber_loss(x,y,model, beta=1.):\n",
    "    y_hat = model(x)\n",
    "    return torch.nn.SmoothL1Loss(beta=beta)(y_hat, y)\n",
    "\n",
    "#set_postfix loss print\n",
    "pbar = tqdm.tqdm(range(1000))\n",
    "for i in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    loss = huber_loss(x_tr,y_tr,huber_model)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_postfix(loss=f'{loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botorch\n",
    "from botorch.models import SingleTaskGP, FixedNoiseGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "import gpytorch\n",
    "\n",
    "# Convert data to torch tensors\n",
    "X_t_tensor = torch.tensor(X_t, dtype=torch.double)\n",
    "Y_t_tensor = torch.tensor(Y_t, dtype=torch.double)\n",
    "\n",
    "#gpytorch fixed kernel parameters\n",
    "mean_module = gpytorch.means.ConstantMean()\n",
    "covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "# set values for the covariance function\n",
    "covar_module.base_kernel.lengthscale = 0.5\n",
    "covar_module.outputscale = 1.0\n",
    "\n",
    "bo_model = FixedNoiseGP(X_t_tensor, Y_t_tensor, torch.ones_like(Y_t_tensor)*0.1, mean_module=mean_module, covar_module=covar_module)\n",
    "\n",
    "# Fit the model\n",
    "mll = ExactMarginalLogLikelihood(bo_model.likelihood, bo_model)\n",
    "# botorch.fit_gpytorch_mll(mll)\n",
    "\n",
    "# Test data\n",
    "test_x = torch.tensor(np.linspace(-3, 10, 1000).reshape(-1, 1), dtype=torch.double)\n",
    "\n",
    "# Make predictions\n",
    "bo_model.eval()\n",
    "with torch.no_grad():\n",
    "    posterior = bo_model.posterior(test_x)\n",
    "    pred_y = posterior.mean.squeeze().numpy()\n",
    "    std_y = posterior.variance.sqrt().squeeze().numpy()\n",
    "\n",
    "bo_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:34:54.110062584Z",
     "start_time": "2024-10-30T09:34:53.931099375Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_fine = torch.linspace(-3, 10, 1000).unsqueeze(-1).double()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(X_t, Y_t, 'r.')\n",
    "plt.plot(X_outliers, Y_outliers, 'rx')\n",
    "plt.plot(x_fine, model(x_fine).detach().numpy(),'g--', lw=3, label='GP Model')\n",
    "plt.plot(x_fine, huber_model(x_fine).detach().numpy(),'c--', lw=3, label='Huber GP Model')\n",
    "plt.plot(x_fine, pred_y, 'k--', lw=3, label='BoTorch Model')\n",
    "plt.plot(x_fine, amini_model(x_fine).detach().numpy()[:,0],'m--', lw=3, label='Amini Model')\n",
    "plt.plot(x_fine, F(x_fine).detach().numpy(),'b-')\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "ax.set_ylim(-5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:34:54.114186963Z",
     "start_time": "2024-10-30T09:34:54.110729201Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "# list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PyTorchEstimator(BaseEstimator):\n",
    "    def __init__(self, model, criterion, optimizer_cls, epochs=10, batch_size=5000, lr=0.01):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer_cls = optimizer_cls\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Set model to training mode\n",
    "        self.model.train()\n",
    "        \n",
    "        # Prepare data\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float64)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float64)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        optimizer = self.optimizer_cls(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        #set_postfix loss print\n",
    "        pbar = tqdm.tqdm(range(1000))\n",
    "        for i in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.criterion(X_tensor, y_tensor, self.model)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float64)\n",
    "            outputs = self.model(X_tensor)\n",
    "            if outputs.shape[1] == 4:\n",
    "                return outputs[:,0].numpy().flatten()\n",
    "        \n",
    "        return outputs.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random shuffle X_t, Y_t \n",
    "perm = torch.randperm(X_t_tensor.size(0))\n",
    "X_t = X_t_tensor[perm]\n",
    "Y_t = Y_t_tensor[perm]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import numpy as np\n",
    "\n",
    "# Initialize model, criterion, and optimizer class\n",
    "criterion = amini_loss\n",
    "optimizer_cls = optim.Adam\n",
    "\n",
    "# Wrap the model with our custom estimator\n",
    "amini_estimator = PyTorchEstimator(model=amini_model, criterion=criterion, optimizer_cls=optimizer_cls, epochs=1000)\n",
    "\n",
    "# Run cross-validation\n",
    "amini_scores = cross_val_score(amini_estimator, X_t, Y_t, cv=10, scoring='neg_mean_squared_error', n_jobs=5)\n",
    "print(\"Amini CV scores:\", amini_scores)\n",
    "\n",
    "pred = cross_val_predict(amini_estimator, X_t, Y_t, cv=10, n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "# sort X_t\n",
    "sorted_idx = np.argsort(X_t.flatten())\n",
    "sorted_idx\n",
    "\n",
    "plt.plot(X_t[sorted_idx], Y_t[sorted_idx], 'r.', label='Noisy Observations')\n",
    "plt.plot(X_t[sorted_idx], pred[sorted_idx], 'b-', label='Predicted')\n",
    "plt.plot(X_t[sorted_idx], F(X_t[sorted_idx]), 'g-', label='True')\n",
    "plt.legend()\n",
    "# mse = (pred[sorted_idx] - F(X_t[sorted_idx]).numpy())**2\n",
    "\n",
    "a = pred[sorted_idx]\n",
    "b = F(X_t[sorted_idx]).numpy().squeeze()\n",
    "mse = (a-b)**2\n",
    "plt.title(f'MSE: {np.mean(mse):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, criterion, and optimizer class\n",
    "criterion = loss_fn\n",
    "optimizer_cls = optim.Adam\n",
    "\n",
    "# Wrap the model with our custom estimator\n",
    "gp_estimator = PyTorchEstimator(model=model, criterion=criterion, optimizer_cls=optimizer_cls, epochs=1000)\n",
    "\n",
    "# Run cross-validation\n",
    "gp_scores = cross_val_score(gp_estimator, X_t, Y_t, cv=10, scoring='neg_mean_squared_error', n_jobs=5)\n",
    "print(\"Normal GP CV scores:\", gp_scores)\n",
    "\n",
    "pred = cross_val_predict(gp_estimator, X_t, Y_t, cv=10, n_jobs=5)\n",
    "a = pred[sorted_idx]\n",
    "b = F(X_t[sorted_idx]).numpy().squeeze()\n",
    "mse = (a-b)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "# sort X_t\n",
    "sorted_idx = np.argsort(X_t.flatten())\n",
    "sorted_idx\n",
    "\n",
    "plt.plot(X_t[sorted_idx], Y_t[sorted_idx], 'r.', label='Noisy Observations')\n",
    "plt.plot(X_t[sorted_idx], pred[sorted_idx], 'b-', label='Predicted')\n",
    "plt.plot(X_t[sorted_idx], F(X_t[sorted_idx]), 'g-', label='True')\n",
    "plt.legend()\n",
    "plt.title(f'MSE: {np.mean(mse):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, criterion, and optimizer class\n",
    "criterion = huber_loss\n",
    "optimizer_cls = optim.Adam\n",
    "\n",
    "# Wrap the model with our custom estimator\n",
    "huber_gp_estimator = PyTorchEstimator(model=huber_model, criterion=criterion, optimizer_cls=optimizer_cls, epochs=1000)\n",
    "\n",
    "# Run cross-validation\n",
    "huber_scores = cross_val_score(huber_gp_estimator, X_t, Y_t, cv=10, scoring='neg_mean_squared_error', n_jobs=5)\n",
    "\n",
    "print(\"Huber GP CV scores:\", huber_scores)\n",
    "\n",
    "pred = cross_val_predict(huber_gp_estimator, X_t, Y_t, cv=10, n_jobs=5)\n",
    "((F(X_t).numpy().squeeze() - pred)**2).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "# sort X_t\n",
    "sorted_idx = np.argsort(X_t.flatten())\n",
    "sorted_idx\n",
    "\n",
    "plt.plot(X_t[sorted_idx], Y_t[sorted_idx], 'r.', label='Noisy Observations')\n",
    "plt.plot(X_t[sorted_idx], pred[sorted_idx], 'b-', label='Predicted')\n",
    "plt.plot(X_t[sorted_idx], F(X_t[sorted_idx]), 'g-', label='True')\n",
    "plt.legend()\n",
    "mse = (pred[sorted_idx] - F(X_t[sorted_idx]).numpy().squeeze())**2\n",
    "plt.title(f'MSE: {np.mean(mse):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_scores = cross_val_score(RandomForestRegressor(), X_t, Y_t, cv=10, scoring='neg_mean_squared_error', n_jobs=5)\n",
    "print(\"Random Forest CV scores:\", rf_scores)\n",
    "\n",
    "pred = cross_val_predict(RandomForestRegressor(), X_t, Y_t, cv=10, n_jobs=5)\n",
    "((F(X_t) - pred)**2).shape\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "# sort X_t\n",
    "sorted_idx = np.argsort(X_t.flatten())\n",
    "sorted_idx\n",
    "\n",
    "plt.plot(X_t[sorted_idx], Y_t[sorted_idx], 'r.', label='Noisy Observations')\n",
    "plt.plot(X_t[sorted_idx], pred[sorted_idx], 'b-', label='Predicted')\n",
    "plt.plot(X_t[sorted_idx], F(X_t[sorted_idx]), 'g-', label='True')\n",
    "plt.legend()\n",
    "mse = (pred[sorted_idx] - F(X_t[sorted_idx]).numpy().squeeze())**2\n",
    "plt.title(f'MSE: {np.mean(mse):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "data_dict = {\n",
    "    'Amini': amini_scores,\n",
    "    'Normal GP': gp_scores,\n",
    "    'Huber GP': huber_scores,\n",
    "    'Random Forest': rf_scores \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data_dict) * -1 # flip sign to get MSE\n",
    "sns.boxplot(data=df, width=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:34:55.076771036Z",
     "start_time": "2024-10-30T09:34:55.071008997Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimization of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:34:57.102702075Z",
     "start_time": "2024-10-30T09:34:57.075419515Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = torch.tensor(0.1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
